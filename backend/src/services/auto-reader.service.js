const { getDb } = require('../db');
const config = require('../config');
const pdfService = require('./pdf.service');
const geminiCliService = require('./gemini-cli.service');
const codexCliService = require('./codex-cli.service');
const googleApiService = require('./google-api.service');
const claudeCodeService = require('./claude-code.service');
const s3Service = require('./s3.service');
const { spawn } = require('child_process');
const fs = require('fs').promises;
const path = require('path');
const { cleanLLMResponse } = require('../utils/clean-llm-response');

/**
 * Auto Reader Service - Multi-pass deep reading mode
 *
 * Based on docs/skill.md and docs/note_templates.md
 *
 * Paper Reading (3 passes):
 * 1. é¸Ÿç°æ‰«æ: Basic info, filter pages, 5C evaluation
 * 2. å†…å®¹ç†è§£: Method overview, experiments, key figures
 * 3. æ·±åº¦ç†è§£: Math framework, method details, generate figures
 *
 * Code Analysis (if has code, 3 rounds):
 * 1. ä»“åº“æ¦‚è§ˆ: Structure, entry points, dependencies
 * 2. æ•°æ®æ¥å£: Data format, data flow, config system
 * 3. æ ¸å¿ƒå®ç°: Key methods, implementation details, reproduce guide
 */

// ============== TEXT FIGURE RULES ==============
const TEXT_FIGURE_RULES = `
## å›¾è¡¨ç»˜åˆ¶è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

**ç¦æ­¢ä½¿ç”¨Mermaid**ã€‚æ‰€æœ‰å›¾è¡¨å¿…é¡»ä½¿ç”¨çº¯æ–‡æœ¬/ASCIIå­—ç¬¦ç”»ï¼Œæ”¾åœ¨ \`\`\` ä»£ç å—ä¸­ã€‚

ç»˜åˆ¶è§„åˆ™ï¼š
1. ä½¿ç”¨ Unicode box-drawing å­—ç¬¦ï¼šâ”Œ â” â”” â”˜ â”‚ â”€ â”œ â”¤ â”¬ â”´ â”¼ â–¶ â–¼ â—€ â–²
2. ç”¨ç®­å¤´è¡¨ç¤ºæ•°æ®æµï¼šâ”€â”€â”€â”€â–¶  Â·Â·Â·Â·â–¶ï¼ˆè™šçº¿ï¼‰  â•â•â•â•â–¶ï¼ˆç²—çº¿ï¼‰
3. ç”¨æ–¹æ¡†è¡¨ç¤ºæ¨¡å—ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â” â”‚ æ¨¡å— â”‚ â””â”€â”€â”€â”€â”€â”€â”˜
4. å¯ä»¥ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾
5. ä¿æŒå¯¹é½å’Œç¾è§‚
6. å›¾è¡¨å®½åº¦ä¸è¶…è¿‡80å­—ç¬¦

ç¤ºä¾‹ï¼š
\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input   â”‚â”€â”€â”€â”€â–¶â”‚ Process  â”‚â”€â”€â”€â”€â–¶â”‚  Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Side Eff â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`
`;

// ============== PROMPTS FOR PAPER READING ==============

const PAPER_PASS_1_PROMPT = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡é˜…è¯»åŠ©æ‰‹ã€‚è¿™æ˜¯ç¬¬ä¸€è½®é˜…è¯»ï¼ˆå¿«é€Ÿæ¦‚è§ˆï¼‰ã€‚

## ä»»åŠ¡
1. è¯»å–æ ‡é¢˜ã€æ‘˜è¦ã€å¼•è¨€
2. æ‰«æç« èŠ‚æ ‡é¢˜ï¼ˆä¸è¯»å†…å®¹ï¼‰
3. è¯»ç»“è®º
4. æ‰«æå‚è€ƒæ–‡çŒ®

## è¾“å‡ºè¦æ±‚
- åªè¾“å‡ºJSONä»£ç å—ï¼Œä¸è¦åŒ…å«ä»»ä½•å¼€åœºç™½æˆ–è¯´æ˜æ€§æ–‡å­—
- ä¸è¦è¯´"æˆ‘å·²å®Œæˆ..."ä¹‹ç±»çš„è¯
- summary å­—æ®µç”¨ä¸­æ–‡æ’°å†™ï¼Œ200-300å­—ï¼Œæ¦‚æ‹¬è®ºæ–‡çš„é—®é¢˜ã€æ–¹æ³•ã€æ ¸å¿ƒç»“æœ

è¯·ç›´æ¥æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š

\`\`\`json
{
  "title": "è®ºæ–‡å®Œæ•´æ ‡é¢˜",
  "paper_type": "å®è¯/ç†è®º/ç³»ç»Ÿ/ç»¼è¿°",
  "venue": "å‘è¡¨venueï¼ˆå¦‚æœèƒ½è¯†åˆ«ï¼‰",
  "has_code": true/false,
  "code_url": "https://github.com/... æˆ– null",
  "core_contribution": "ç”¨1-2å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè´¡çŒ®",
  "summary": "200-300å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œæ¶µç›–è®ºæ–‡è¦è§£å†³çš„é—®é¢˜ã€é‡‡ç”¨çš„æ–¹æ³•ã€ä¸»è¦å®éªŒç»“æœå’Œå…³é”®ç»“è®º",
  "key_pages": "å¦‚ p3-5æ–¹æ³•, p6-8å®éªŒ",
  "skip_pages": "å¦‚ p9-10é™„å½•, p2ç›¸å…³å·¥ä½œè¯¦ç»†",
  "key_figures": [1, 3, 5]
}
\`\`\``;

const PAPER_PASS_2_PROMPT = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡é˜…è¯»åŠ©æ‰‹ã€‚è¿™æ˜¯ç¬¬äºŒè½®é˜…è¯»ï¼ˆå†…å®¹ç†è§£ï¼‰ã€‚

## èƒŒæ™¯ä¿¡æ¯
ç¬¬ä¸€è½®ç¬”è®°ï¼š
{previous_notes}

## ä»»åŠ¡
èšç„¦é˜…è¯»ç¬¬ä¸€è½®æ ‡è®°çš„å…³é”®é¡µé¢ï¼ŒæŠŠæ¡è®ºæ–‡å†…å®¹ä½†ä¸æ·±å…¥ç»†èŠ‚ã€‚

## è¾“å‡ºè¦æ±‚
- ç›´æ¥è¾“å‡ºMarkdownå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å¼€åœºç™½æˆ–è¯´æ˜æ€§æ–‡å­—
- ä¸è¦ä½¿ç”¨<details>æˆ–<summary>æ ‡ç­¾
- ä¸è¦è¯´"æˆ‘å·²å®Œæˆ..."ä¹‹ç±»çš„è¯
- **é‡è¦**: å¿…é¡»ä¸ºè®ºæ–‡ä¸­çš„å…³é”®å›¾è¡¨ç»˜åˆ¶å¯è§†åŒ–å›¾ï¼Œå¸®åŠ©è¯»è€…ç†è§£

${TEXT_FIGURE_RULES}

è¯·ç›´æ¥è¾“å‡ºä»¥ä¸‹Markdownæ ¼å¼ï¼š

### æ ¸å¿ƒé—®é¢˜
[è®ºæ–‡è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿä¸ºä»€ä¹ˆè¿™ä¸ªé—®é¢˜é‡è¦ï¼Ÿ]

### æ–¹æ³•æ¦‚è¿°
[ç”¨è‡ªå·±çš„è¯æè¿°æ–¹æ³•ï¼Œä¸è¶…è¿‡ä¸€æ®µ]

### å…³é”®å›¾è¡¨å¤ç°

å¯¹äºè®ºæ–‡ä¸­æ¯ä¸ªé‡è¦çš„å›¾è¡¨ï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºå…¶å¯è§†åŒ–å¤ç°ï¼š

**Figure X: [å›¾æ ‡é¢˜]**

[è¿™ä¸ªå›¾è¯´æ˜äº†ä»€ä¹ˆï¼Œ1-2å¥è¯]

ä½¿ç”¨ASCIIå­—ç¬¦ç”»å¤ç°æµç¨‹å›¾/æ¶æ„å›¾ï¼š
\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input   â”‚â”€â”€â”€â”€â–¶â”‚ Process  â”‚â”€â”€â”€â”€â–¶â”‚  Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

**Table Y: [è¡¨æ ‡é¢˜]**

[è¿™ä¸ªè¡¨çš„å…³é”®å‘ç°]

ä½¿ç”¨Markdownè¡¨æ ¼å¤ç°å…³é”®æ•°æ®ï¼š
| Method | Metric1 | Metric2 | Metric3 |
|--------|---------|---------|---------|
| Baseline | 32.1 | 45.2 | 38.5 |
| **Ours** | **45.3** | **58.7** | **51.2** |

### æ–¹æ³•æµç¨‹å›¾

ä½¿ç”¨ASCIIå­—ç¬¦ç”»ç»˜åˆ¶è®ºæ–‡æ ¸å¿ƒæ–¹æ³•çš„æµç¨‹ï¼š

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Inputâ”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1  â”‚â”€â”€â”€â”€â–¶â”‚  Step 2  â”‚â”€â”€â”€â”€â–¶â”‚  Step 3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                       â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚  Output  â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### å®éªŒè®¾ç½®
- **æ•°æ®é›†**: [åç§°å’Œè§„æ¨¡]
- **åŸºçº¿æ–¹æ³•**: [å¯¹æ¯”æ–¹æ³•åˆ—è¡¨]
- **è¯„ä¼°æŒ‡æ ‡**: [æŒ‡æ ‡åŠå…¶å«ä¹‰]

### ä¸»è¦ç»“æœ
[ç»“æœæ€»ç»“ï¼Œå…³é”®æ•°å­—ï¼Œæœ€å¥½ç”¨è¡¨æ ¼å‘ˆç°]

### å­˜ç–‘ç‚¹
- [ ] [ä¸ç†è§£çš„ç‚¹1]
- [ ] [ä¸ç†è§£çš„ç‚¹2]

### å¾…è¿½è¯»æ–‡çŒ®
- [ ] [é‡è¦å‚è€ƒæ–‡çŒ®1] - [ä¸ºä»€ä¹ˆé‡è¦]
- [ ] [é‡è¦å‚è€ƒæ–‡çŒ®2] - [ä¸ºä»€ä¹ˆé‡è¦]`;

const PAPER_PASS_3_PROMPT = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡é˜…è¯»åŠ©æ‰‹ã€‚è¿™æ˜¯ç¬¬ä¸‰è½®é˜…è¯»ï¼ˆæ·±åº¦ç†è§£ï¼‰ã€‚

## èƒŒæ™¯ä¿¡æ¯
ä¹‹å‰çš„ç¬”è®°ï¼š
{previous_notes}

## ä»»åŠ¡
æ·±å…¥æ–¹æ³•ç»†èŠ‚ï¼Œæ„å»ºæ•°å­¦æ¡†æ¶ï¼Œç»˜åˆ¶è¯¦ç»†çš„ç³»ç»Ÿå›¾ã€‚

## è¾“å‡ºè¦æ±‚
- ç›´æ¥è¾“å‡ºMarkdownå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å¼€åœºç™½æˆ–è¯´æ˜æ€§æ–‡å­—
- ä¸è¦ä½¿ç”¨<details>æˆ–<summary>æ ‡ç­¾
- ä¸è¦è¯´"æˆ‘å·²å®Œæˆ..."ä¹‹ç±»çš„è¯
- æ•°å­¦å…¬å¼ä½¿ç”¨ $...$ æˆ– $$...$$ æ ¼å¼
- **é‡è¦**: å¿…é¡»ç»˜åˆ¶å¤šä¸ªè¯¦ç»†çš„æ¶æ„å›¾æ¥å¸®åŠ©è¯»è€…ç†è§£è®ºæ–‡

${TEXT_FIGURE_RULES}

è¯·ç›´æ¥è¾“å‡ºä»¥ä¸‹Markdownæ ¼å¼ï¼š

---

## æ·±åº¦è§£æ

### æ•°å­¦æ¡†æ¶

**é—®é¢˜å½¢å¼åŒ–**

è®¾è¾“å…¥ç©ºé—´ $\\mathcal{X}$ï¼Œè¾“å‡ºç©ºé—´ $\\mathcal{Y}$ï¼Œç›®æ ‡æ˜¯å­¦ä¹ æ˜ å°„...
[å…·ä½“é—®é¢˜å®šä¹‰]

**å…³é”®å…¬å¼**

$$
[æ ¸å¿ƒå…¬å¼ï¼Œä½¿ç”¨ LaTeX æ ¼å¼]
$$

å…¬å¼è§£è¯»ï¼š[é€é¡¹è§£é‡Šå…¬å¼ä¸­çš„ç¬¦å·å«ä¹‰]

### ç³»ç»Ÿæ¶æ„æ€»è§ˆ

ä½¿ç”¨ASCIIå­—ç¬¦ç”»ç»˜åˆ¶è®ºæ–‡çš„æ•´ä½“ç³»ç»Ÿæ¶æ„ï¼š

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  System Overview                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚  Input   â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚       â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Module 1 â”‚â”€â–¶â”‚ Module 2 â”‚â”€â–¶â”‚ Module 3 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       :                           â–¼              â”‚
â”‚       Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â–¶â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                             â”‚  Output  â”‚         â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### æ ¸å¿ƒç®—æ³•æµç¨‹

ä½¿ç”¨ASCIIå­—ç¬¦ç”»ç»˜åˆ¶æ ¸å¿ƒç®—æ³•çš„è¯¦ç»†æ­¥éª¤ï¼š

\`\`\`
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Start  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 1: Desc â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 2: Desc â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Conditionâ”‚
    â””â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”˜
   Yes â”‚   â”‚ No
       â–¼   â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚Brch Aâ”‚ â”‚Brch Bâ”‚
  â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Merge  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   End   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### æ•°æ®æµå›¾

å±•ç¤ºæ•°æ®åœ¨ç³»ç»Ÿä¸­çš„æµåŠ¨è¿‡ç¨‹ï¼š

\`\`\`
[ Data Preprocessing ]          [ Model Processing ]          [ Post-processing ]
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw  â”‚â”€â–¶â”‚ Clean  â”‚â”€â–¶â”‚Feature â”‚â”€â–¶â”‚ Encoder â”‚â”€â–¶â”‚  Core  â”‚â”€â–¶â”‚ Decoder â”‚â”€â–¶â”‚ Output â”‚
â”‚ Data â”‚  â”‚        â”‚  â”‚Extract â”‚  â”‚         â”‚  â”‚Compute â”‚  â”‚         â”‚  â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### æ–¹æ³•æ·±åº¦è§£æ

[è¯¦ç»†æ–¹æ³•æè¿°ï¼Œæ¯ä¸ªç»„ä»¶çš„ä½œç”¨ï¼Œå…³é”®è®¾è®¡é€‰æ‹©]

### åˆ›æ–°ç‚¹åˆ†æ

1. **[åˆ›æ–°ç‚¹1]**: [æ„ä¹‰å’Œä»·å€¼]
2. **[åˆ›æ–°ç‚¹2]**: [æ„ä¹‰å’Œä»·å€¼]

### å±€é™æ€§ä¸å‡è®¾

- **éšå«å‡è®¾**: [è®ºæ–‡æœªæ˜è¯´ä½†å¿…é¡»æˆç«‹çš„å‡è®¾]
- **é€‚ç”¨èŒƒå›´**: [æ–¹æ³•åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹æœ‰æ•ˆ]
- **æ½œåœ¨é—®é¢˜**: [å¯èƒ½çš„å¤±æ•ˆåœºæ™¯]

### ç›¸å…³å·¥ä½œå¯¹æ¯”

- **[ç›¸å…³å·¥ä½œ1]**: [åŒºåˆ«å’Œè”ç³»]
- **[ç›¸å…³å·¥ä½œ2]**: [åŒºåˆ«å’Œè”ç³»]

### æœªæ¥å·¥ä½œæƒ³æ³•

1. [æƒ³æ³•1]
2. [æƒ³æ³•2]

### æ ¸å¿ƒæµç¨‹æ€»ç»“

ç”¨ä¸€æ®µæ–‡å­—æè¿°è®ºæ–‡çš„æ ¸å¿ƒæµç¨‹ï¼šä»è¾“å…¥åˆ°è¾“å‡ºï¼Œç»è¿‡å“ªäº›å…³é”®æ­¥éª¤ï¼Œæ¯æ­¥åšä»€ä¹ˆã€‚`;

// ============== V3 PROMPTS: 2-PASS DEEP ANALYSIS ==============

const V3_PASS_1_PROMPT = `ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¿™æ˜¯ç¬¬ä¸€è½®åˆ†æï¼ˆè¡¨å±‚åˆ†æï¼‰ã€‚

## ä»»åŠ¡
ä»”ç»†é˜…è¯»è®ºæ–‡ï¼Œå®Œæˆä»¥ä¸‹åˆ†æï¼š

1. **ä»»åŠ¡å®šä¹‰**ï¼šè¿™ç¯‡è®ºæ–‡è§£å†³ä»€ä¹ˆé—®é¢˜/ä»»åŠ¡ï¼Ÿ
2. **é¢†åŸŸç°çŠ¶**ï¼šè¿™ä¸ªé—®é¢˜æ‰€åœ¨é¢†åŸŸçš„å‘å±•çŠ¶å†µå¦‚ä½•ï¼Ÿç®€è¿°ç›¸å…³å·¥ä½œ
3. **è¾“å…¥å®šä¹‰**ï¼šè¿™ä¸ªä»»åŠ¡çš„è¾“å…¥æ˜¯ä»€ä¹ˆï¼Ÿæ ¼å¼ã€ç»´åº¦ã€è¯­ä¹‰
4. **è¾“å‡ºå®šä¹‰**ï¼šè¿™ä¸ªä»»åŠ¡çš„è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿæ ¼å¼ã€ç»´åº¦ã€è¯­ä¹‰
5. **æ–¹æ³•æ¦‚è§ˆ**ï¼šç”¨ä¸€å¼ æ–‡æœ¬å›¾æè¿°è®ºæ–‡çš„æ•´ä½“æ–¹æ³•æµç¨‹

## è¾“å‡ºè¦æ±‚
- ç›´æ¥è¾“å‡ºMarkdownå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å¼€åœºç™½æˆ–è¯´æ˜æ€§æ–‡å­—
- ä¸è¦è¯´"æˆ‘å·²å®Œæˆ..."ä¹‹ç±»çš„è¯
- ä½¿ç”¨ä¸­æ–‡è¾“å‡º

${TEXT_FIGURE_RULES}

è¯·ç›´æ¥æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ä»»åŠ¡å®šä¹‰

**æ ¸å¿ƒé—®é¢˜**ï¼š[ä¸€å¥è¯æè¿°è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜]

**é—®é¢˜èƒŒæ™¯**ï¼š
[2-3å¥è¯è§£é‡Šä¸ºä»€ä¹ˆè¿™ä¸ªé—®é¢˜é‡è¦ï¼Œæœ‰ä»€ä¹ˆå®é™…åº”ç”¨åœºæ™¯]

**å½¢å¼åŒ–å®šä¹‰**ï¼š
- ç»™å®šï¼š[è¾“å…¥çš„å½¢å¼åŒ–æè¿°]
- ç›®æ ‡ï¼š[éœ€è¦å­¦ä¹ /ä¼˜åŒ–çš„ç›®æ ‡]
- çº¦æŸï¼š[å¦‚æœæœ‰çš„è¯]

## é¢†åŸŸç°çŠ¶

**å‘å±•é˜¶æ®µ**ï¼š[èŒèŠ½æœŸ/æˆé•¿æœŸ/æˆç†ŸæœŸ/å˜é©æœŸ]

**ä¸»æµæ–¹æ³•æ´¾ç³»**ï¼š
1. **[æ´¾ç³»1åç§°]**ï¼š[æ ¸å¿ƒæ€æƒ³]ï¼Œä»£è¡¨ä½œï¼š[è®ºæ–‡å]
2. **[æ´¾ç³»2åç§°]**ï¼š[æ ¸å¿ƒæ€æƒ³]ï¼Œä»£è¡¨ä½œï¼š[è®ºæ–‡å]
3. **[æ´¾ç³»3åç§°]**ï¼š[æ ¸å¿ƒæ€æƒ³]ï¼Œä»£è¡¨ä½œï¼š[è®ºæ–‡å]

**å½“å‰ç“¶é¢ˆ**ï¼š
- [ç“¶é¢ˆ1]
- [ç“¶é¢ˆ2]

**æœ¬æ–‡å®šä½**ï¼š[æœ¬æ–‡å±äºå“ªä¸ªæ´¾ç³»ï¼Œæˆ–å¼€åˆ›äº†ä»€ä¹ˆæ–°æ–¹å‘]

## è¾“å…¥è§„æ ¼

**æ•°æ®ç±»å‹**ï¼š[æ–‡æœ¬/å›¾åƒ/è¡¨æ ¼/å¤šæ¨¡æ€/...]

**å½¢å¼åŒ–è¡¨ç¤º**ï¼š
\`\`\`
è¾“å…¥ X âˆˆ [ç»´åº¦æè¿°]
- å­—æ®µ1: [ç±»å‹] - [è¯­ä¹‰è¯´æ˜]
- å­—æ®µ2: [ç±»å‹] - [è¯­ä¹‰è¯´æ˜]
\`\`\`

**ç¤ºä¾‹**ï¼š
\`\`\`
[ä¸€ä¸ªå…·ä½“çš„è¾“å…¥ç¤ºä¾‹]
\`\`\`

## è¾“å‡ºè§„æ ¼

**è¾“å‡ºç±»å‹**ï¼š[åˆ†ç±»/å›å½’/ç”Ÿæˆ/æ£€ç´¢/...]

**å½¢å¼åŒ–è¡¨ç¤º**ï¼š
\`\`\`
è¾“å‡º Y âˆˆ [ç»´åº¦æè¿°]
- å­—æ®µ1: [ç±»å‹] - [è¯­ä¹‰è¯´æ˜]
\`\`\`

**ç¤ºä¾‹**ï¼š
\`\`\`
[å¯¹åº”è¾“å…¥çš„è¾“å‡ºç¤ºä¾‹]
\`\`\`

## æ–¹æ³•æ¦‚è§ˆå›¾

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        [è®ºæ–‡æ–¹æ³•åç§°]                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [Input]                                                         â”‚
â”‚     â”‚                                                            â”‚
â”‚     â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ Stage 1  â”‚â”€â”€â”€â”€â–¶â”‚ Stage 2  â”‚â”€â”€â”€â”€â–¶â”‚ Stage 3  â”‚                 â”‚
â”‚  â”‚ [æè¿°]   â”‚     â”‚ [æè¿°]   â”‚     â”‚ [æè¿°]   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                         â”‚                â”‚                       â”‚
â”‚                         â–¼                â–¼                       â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                   â”‚ [è¾…åŠ©]   â”‚     â”‚ [Output] â”‚                 â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

**æµç¨‹è¯´æ˜**ï¼š
1. **[é˜¶æ®µ1]**ï¼š[åšä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆ]
2. **[é˜¶æ®µ2]**ï¼š[åšä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆ]
3. **[é˜¶æ®µ3]**ï¼š[åšä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆ]

## å…³é”®æŠ€æœ¯ç‚¹

- **[æŠ€æœ¯ç‚¹1]**ï¼š[ä¸€å¥è¯è§£é‡Š]
- **[æŠ€æœ¯ç‚¹2]**ï¼š[ä¸€å¥è¯è§£é‡Š]
- **[æŠ€æœ¯ç‚¹3]**ï¼š[ä¸€å¥è¯è§£é‡Š]`;

const V3_PASS_2_PROMPT = `ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶å’Œç®—æ³•å·¥ç¨‹å¸ˆã€‚è¿™æ˜¯ç¬¬äºŒè½®åˆ†æï¼ˆæ·±å±‚åˆ†æï¼‰ã€‚

## èƒŒæ™¯ä¿¡æ¯
ç¬¬ä¸€è½®åˆ†æç»“æœï¼š
{previous_notes}

## ä»»åŠ¡
åŸºäºç¬¬ä¸€è½®çš„è¡¨å±‚åˆ†æï¼Œè¿›è¡Œæ›´æ·±å…¥çš„åˆ†æï¼š

1. **æœ€å°å¤ç°**ï¼šè®¾è®¡ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ä»£ç å®ç°ï¼Œå±•ç¤ºè®ºæ–‡æ ¸å¿ƒæ–¹æ³•
2. **æ•°å­¦æ¡†æ¶**ï¼šç”¨ç¬¬ä¸€æ€§åŸç†æ€è€ƒè¿™ä¸ªé—®é¢˜çš„æ•°å­¦æœ¬è´¨
3. **æ‰¹åˆ¤æ€§åˆ†æ**ï¼šæ‰¾å‡ºè®ºæ–‡çš„æ½œåœ¨é—®é¢˜å’Œå±€é™
4. **æœªæ¥æ–¹å‘**ï¼šåŸºäºä½ çš„åˆ†æï¼Œæå‡ºå¯èƒ½çš„æ”¹è¿›æ–¹å‘

## è¾“å‡ºè¦æ±‚
- ç›´æ¥è¾“å‡ºMarkdownå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å¼€åœºç™½æˆ–è¯´æ˜æ€§æ–‡å­—
- ä»£ç ä½¿ç”¨ Pythonï¼Œè¿½æ±‚ç®€æ´å¯è¿è¡Œ
- ä½¿ç”¨ä¸­æ–‡è¾“å‡º
- æ•°å­¦å…¬å¼ä½¿ç”¨ $...$ æˆ– $$...$$ æ ¼å¼

è¯·ç›´æ¥æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

---

## æœ€å°å¤ç°å®ç°

### è®¾è®¡æ€è·¯

[ç”¨2-3å¥è¯è§£é‡Šä½ çš„ç®€åŒ–ç­–ç•¥ï¼šä¿ç•™äº†ä»€ä¹ˆæ ¸å¿ƒï¼Œçœç•¥äº†ä»€ä¹ˆç»†èŠ‚]

### æ¨¡æ‹Ÿæ•°æ®

\`\`\`python
import numpy as np
import torch
import torch.nn as nn

# æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
def generate_mock_data(batch_size=4):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Œç¬¦åˆè®ºæ–‡è¾“å…¥æ ¼å¼
    """
    # [æ ¹æ®è®ºæ–‡è¾“å…¥è§„æ ¼è®¾è®¡]
    return {
        'input_field1': ...,
        'input_field2': ...,
    }

# ç¤ºä¾‹
mock_data = generate_mock_data()
print("Input shape:", ...)
\`\`\`

### æ ¸å¿ƒæ–¹æ³•å®ç°

\`\`\`python
class SimplifiedMethod(nn.Module):
    """
    [è®ºæ–‡æ–¹æ³•å] çš„ç®€åŒ–å®ç°

    ç®€åŒ–ç‚¹ï¼š
    - [çœç•¥äº†ä»€ä¹ˆ]
    - [ç®€åŒ–äº†ä»€ä¹ˆ]

    ä¿ç•™ç‚¹ï¼š
    - [ä¿ç•™çš„æ ¸å¿ƒæœºåˆ¶1]
    - [ä¿ç•™çš„æ ¸å¿ƒæœºåˆ¶2]
    """

    def __init__(self, ...):
        super().__init__()
        # [æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–]

    def forward(self, x):
        # Step 1: [é˜¶æ®µ1åç§°]
        # [å®ç°]

        # Step 2: [é˜¶æ®µ2åç§°]
        # [å®ç°]

        return output

# è¿è¡Œç¤ºä¾‹
model = SimplifiedMethod(...)
output = model(mock_data)
print("Output shape:", output.shape)
\`\`\`

### å®Œæ•´è¿è¡Œè„šæœ¬

\`\`\`python
# å®Œæ•´çš„å¯è¿è¡Œè„šæœ¬
if __name__ == "__main__":
    # 1. å‡†å¤‡æ•°æ®
    data = generate_mock_data(batch_size=4)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = SimplifiedMethod(...)

    # 3. å‰å‘ä¼ æ’­
    output = model(data)

    # 4. éªŒè¯è¾“å‡º
    print(f"Input: {data}")
    print(f"Output: {output}")
    print(f"Output shape: {output.shape}")
\`\`\`

---

## æ•°å­¦æ¡†æ¶åˆ†æ

### é—®é¢˜çš„æ•°å­¦æœ¬è´¨

**æŠ½è±¡å½¢å¼åŒ–**ï¼š

$$
[ç”¨æœ€æŠ½è±¡çš„æ•°å­¦è¯­è¨€æè¿°è¿™ä¸ªé—®é¢˜]
$$

å…¶ä¸­ï¼š
- $[ç¬¦å·1]$ï¼š[å«ä¹‰]
- $[ç¬¦å·2]$ï¼š[å«ä¹‰]

### ä¼˜åŒ–ç›®æ ‡åˆ†è§£

**è®ºæ–‡çš„æŸå¤±å‡½æ•°**ï¼š

$$
\\mathcal{L} = [è®ºæ–‡çš„æŸå¤±å‡½æ•°]
$$

**åˆ†è§£åˆ†æ**ï¼š
- **é¡¹1** $[L_1]$ï¼š[ä½œç”¨] - [ä¼˜åŒ–ä»€ä¹ˆ]
- **é¡¹2** $[L_2]$ï¼š[ä½œç”¨] - [ä¼˜åŒ–ä»€ä¹ˆ]

### æœ¬æ–‡åœ¨æ¡†æ¶ä¸­çš„ä½ç½®

\`\`\`
                    [é—®é¢˜ç©ºé—´çš„ç»´åº¦åˆ’åˆ†]

        ç»´åº¦1ï¼š[æŸä¸ªæŠ€æœ¯é€‰æ‹©ç»´åº¦]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
        â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚   â”‚ æ–¹æ³•A â”‚       â”‚ æ–¹æ³•B â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”˜
ç»´åº¦2   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
[æŸä¸ª   â”‚              â”‚ æœ¬æ–‡æ–¹æ³• â”‚ â˜…
æŠ€æœ¯    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
é€‰æ‹©]   â”‚
        â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚       â”‚ æ–¹æ³•C â”‚
        â–¼       â””â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

**å®šä½åˆ†æ**ï¼š
- æœ¬æ–‡åœ¨ [ç»´åº¦1] ä¸Šé€‰æ‹©äº† [é€‰é¡¹]ï¼Œå› ä¸º [åŸå› ]
- æœ¬æ–‡åœ¨ [ç»´åº¦2] ä¸Šé€‰æ‹©äº† [é€‰é¡¹]ï¼Œå› ä¸º [åŸå› ]

---

## æ‰¹åˆ¤æ€§åˆ†æ

### ä»ç¬¬ä¸€æ€§åŸç†çœ‹é—®é¢˜

**è¿™ä¸ªé—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ**
[ç”¨ä¸€æ®µè¯ä»æœ€åŸºæœ¬çš„åŸç†å‡ºå‘åˆ†æè¿™ä¸ªé—®é¢˜]

**è®ºæ–‡çš„å‡è®¾æ˜¯å¦åˆç†ï¼Ÿ**
1. **å‡è®¾1**ï¼š[è®ºæ–‡éšå«çš„å‡è®¾] - [æ˜¯å¦åˆç†ï¼Œä¸ºä»€ä¹ˆ]
2. **å‡è®¾2**ï¼š[è®ºæ–‡éšå«çš„å‡è®¾] - [æ˜¯å¦åˆç†ï¼Œä¸ºä»€ä¹ˆ]

### æ½œåœ¨é—®é¢˜

| é—®é¢˜ | ä¸¥é‡ç¨‹åº¦ | è¯´æ˜ |
|------|---------|------|
| [é—®é¢˜1] | ğŸ”´/ğŸŸ¡/ğŸŸ¢ | [å…·ä½“è¯´æ˜] |
| [é—®é¢˜2] | ğŸ”´/ğŸŸ¡/ğŸŸ¢ | [å…·ä½“è¯´æ˜] |
| [é—®é¢˜3] | ğŸ”´/ğŸŸ¡/ğŸŸ¢ | [å…·ä½“è¯´æ˜] |

### å®éªŒè®¾è®¡çš„å±€é™

- **æ•°æ®é›†åå·®**ï¼š[å¦‚æœæœ‰]
- **è¯„ä¼°æŒ‡æ ‡ç›²åŒº**ï¼š[å¦‚æœæœ‰]
- **å¯¹æ¯”æ–¹æ³•é€‰æ‹©**ï¼š[å¦‚æœæœ‰]

---

## æœªæ¥æ–¹å‘

### çŸ­æœŸæ”¹è¿›ï¼ˆå·¥ç¨‹å±‚é¢ï¼‰

1. **[æ”¹è¿›1æ ‡é¢˜]**
   - ç°çŠ¶ï¼š[å½“å‰é—®é¢˜]
   - æ–¹æ¡ˆï¼š[æ”¹è¿›æ–¹æ³•]
   - é¢„æœŸï¼š[é¢„æœŸæ•ˆæœ]

2. **[æ”¹è¿›2æ ‡é¢˜]**
   - ç°çŠ¶ï¼š[å½“å‰é—®é¢˜]
   - æ–¹æ¡ˆï¼š[æ”¹è¿›æ–¹æ³•]
   - é¢„æœŸï¼š[é¢„æœŸæ•ˆæœ]

### é•¿æœŸæ–¹å‘ï¼ˆç ”ç©¶å±‚é¢ï¼‰

1. **[æ–¹å‘1]**ï¼š[ä¸ºä»€ä¹ˆé‡è¦ï¼Œå¤§è‡´æ€è·¯]
2. **[æ–¹å‘2]**ï¼š[ä¸ºä»€ä¹ˆé‡è¦ï¼Œå¤§è‡´æ€è·¯]

### è·¨é¢†åŸŸå¯å‘

- **å¯å€Ÿé‰´åˆ° [é¢†åŸŸ1]**ï¼š[å¦‚ä½•å€Ÿé‰´]
- **å¯å€Ÿé‰´åˆ° [é¢†åŸŸ2]**ï¼š[å¦‚ä½•å€Ÿé‰´]

---

## ä¸€å¥è¯æ€»ç»“

> [ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®å’Œä½ çš„è¯„ä»·]`;

// ============== PROMPT FOR CODE ANALYSIS (SINGLE ROUND) ==============

const CODE_ANALYSIS_PROMPT = `You are a code analysis assistant. Analyze the repository content provided below.

IMPORTANT RULES:
- DO NOT use any tools or function calls
- DO NOT try to read additional files
- ONLY analyze the content already provided in this prompt
- Respond in Chinese
- Keep your response under 500 words

Based on the file contents provided below, output your analysis in this format:

## åŸºæœ¬ä¿¡æ¯
- è¯­è¨€/æ¡†æ¶: [from README or code files]
- å…¥å£æ–‡ä»¶: [main entry point]
- æ ¸å¿ƒç›®å½•: [key directories]

## è¿è¡Œå‘½ä»¤
- å®‰è£…: [installation command]
- è®­ç»ƒ: [training command if ML project]

## æ ¸å¿ƒæ¨¡å‹
- æ¨¡å‹ç±»: [main model class and file]
- å…³é”®å®ç°: [1-2 key technical points]

## å¤ç°æ³¨æ„
- [1-2 important notes for reproduction]`;

class AutoReaderService {
  constructor() {
    this.processingDir = config.reader?.tmpDir || path.join(__dirname, '..', '..', 'processing');
    this.ensureProcessingDir();
  }

  async ensureProcessingDir() {
    try {
      await fs.mkdir(this.processingDir, { recursive: true });
    } catch (e) {
      // Directory exists or cannot be created
    }
  }

  /**
   * Process a document in auto_reader mode (multi-pass)
   */
  async processDocument(item, options = {}) {
    const { documentId, s3Key, title, analysisProvider } = item;
    // Use provided codeUrl if available (from request or document)
    let providedCodeUrl = item.codeUrl;
    let tempFilePath = null;
    const notesFilePath = path.join(this.processingDir, `${documentId}_notes.md`);
    // Resolve which provider service to use
    this._currentProvider = this._resolveProvider(analysisProvider);

    try {
      await this.ensureProcessingDir();
      console.log(`[AutoReader] Starting multi-pass processing: ${title} (ID: ${documentId})`);

      // Step 1: Prepare PDF
      const pdfInfo = await pdfService.preparePdfForProcessing(s3Key);
      tempFilePath = pdfInfo.filePath;

      console.log(`[AutoReader] PDF prepared: ${pdfInfo.pageCount} pages`);

      // Initialize notes file with template header
      await this.initNotesFile(notesFilePath, title, documentId);

      // Step 2: Pass 1 - é¸Ÿç°æ‰«æ
      console.log('[AutoReader] === ç¬¬ä¸€è½®ï¼šé¸Ÿç°æ‰«æ ===');
      const pass1Result = await this.executePass(tempFilePath, PAPER_PASS_1_PROMPT, notesFilePath, 1);

      // Parse pass 1 result and format according to template
      const pass1Data = this.parsePass1Result(pass1Result.text);
      await this.appendPass1Notes(notesFilePath, pass1Data, pass1Result.text);

      // Use detected code URL or provided one
      let hasCode = pass1Data.has_code || !!providedCodeUrl;
      let codeUrl = pass1Data.code_url || providedCodeUrl;

      // Step 3: Pass 2 - å†…å®¹ç†è§£
      console.log('[AutoReader] === ç¬¬äºŒè½®ï¼šå†…å®¹ç†è§£ ===');
      const currentNotes = await fs.readFile(notesFilePath, 'utf-8');
      const pass2Prompt = PAPER_PASS_2_PROMPT.replace('{previous_notes}', currentNotes);
      const pass2Result = await this.executePass(tempFilePath, pass2Prompt, notesFilePath, 2);
      await this.appendToNotesFile(notesFilePath, '\n\n---\n\n## ç¬¬äºŒè½®ç¬”è®°\n\n' + cleanLLMResponse(pass2Result.text));

      // Step 4: Pass 3 - æ·±åº¦ç†è§£
      console.log('[AutoReader] === ç¬¬ä¸‰è½®ï¼šæ·±åº¦ç†è§£ ===');
      const updatedNotes = await fs.readFile(notesFilePath, 'utf-8');
      const pass3Prompt = PAPER_PASS_3_PROMPT.replace('{previous_notes}', updatedNotes);
      const pass3Result = await this.executePass(tempFilePath, pass3Prompt, notesFilePath, 3);
      await this.appendToNotesFile(notesFilePath, '\n\n' + cleanLLMResponse(pass3Result.text));

      // Step 5: Generate final paper notes
      let finalNotes = await fs.readFile(notesFilePath, 'utf-8');

      // Step 8: If has code, fetch README and summarize it (non-fatal)
      if (hasCode && codeUrl) {
        console.log(`[AutoReader] === è·å–å¹¶æ‘˜è¦ä»£ç README: ${codeUrl} ===`);
        try {
          const codeReadme = await this.fetchGitHubReadme(codeUrl);
          if (codeReadme) {
            // Summarize README using Gemini CLI
            const readmeSummary = await this.summarizeReadme(codeReadme, codeUrl, title);
            if (readmeSummary) {
              finalNotes += '\n\n---\n\n## ä»£ç ä»“åº“æ¦‚è§ˆ\n\n';
              finalNotes += `**ä»“åº“åœ°å€**: [${codeUrl}](${codeUrl})\n\n`;
              finalNotes += readmeSummary;
              finalNotes += '\n\n*ç‚¹å‡»"ä»£ç åˆ†æ"æŒ‰é’®è·å–è¯¦ç»†çš„ä»£ç è§£è¯»*\n';
            }
          }
        } catch (readmeError) {
          console.log(`[AutoReader] README processing failed (non-fatal):`, readmeError.message);
        }
      }

      // Step 6: Upload paper notes to S3
      const paperNotesS3Key = await this.uploadNotesToS3(finalNotes, documentId, title, 'paper_notes');
      console.log(`[AutoReader] Paper notes uploaded to S3: ${paperNotesS3Key}`);
      console.log(`[AutoReader] Processing complete for: ${title}`);

      return {
        notesS3Key: paperNotesS3Key,
        codeNotesS3Key: null,  // Code notes only available via manual trigger
        pageCount: pdfInfo.pageCount,
        hasCode,
        codeUrl,
        readerMode: 'auto_reader',
      };
    } finally {
      // Cleanup
      if (tempFilePath) {
        await pdfService.cleanupTmpFile(tempFilePath);
      }
      try {
        await fs.unlink(notesFilePath);
      } catch (e) { /* ignore */ }
    }
  }

  /**
   * Process document in v2 mode: same 3-pass reading with text figures (no mermaid)
   */
  async processDocumentV2(item, options = {}) {
    const { documentId, s3Key, title, analysisProvider } = item;
    let providedCodeUrl = item.codeUrl;
    let tempFilePath = null;
    const notesFilePath = path.join(this.processingDir, `${documentId}_notes.md`);
    this._currentProvider = this._resolveProvider(analysisProvider);

    try {
      await this.ensureProcessingDir();
      console.log(`[AutoReaderV2] Starting multi-pass processing: ${title} (ID: ${documentId})`);

      // Step 1: Prepare PDF
      const pdfInfo = await pdfService.preparePdfForProcessing(s3Key);
      tempFilePath = pdfInfo.filePath;
      console.log(`[AutoReaderV2] PDF prepared: ${pdfInfo.pageCount} pages`);

      // Initialize notes file
      await this.initNotesFile(notesFilePath, title, documentId);

      // Step 2: Pass 1
      console.log('[AutoReaderV2] === ç¬¬ä¸€è½®ï¼šå¿«é€Ÿæ¦‚è§ˆ ===');
      const pass1Result = await this.executePass(tempFilePath, PAPER_PASS_1_PROMPT, notesFilePath, 1);
      const pass1Data = this.parsePass1Result(pass1Result.text);
      await this.appendPass1Notes(notesFilePath, pass1Data, pass1Result.text);

      let hasCode = pass1Data.has_code || !!providedCodeUrl;
      let codeUrl = pass1Data.code_url || providedCodeUrl;

      // Step 3: Pass 2
      console.log('[AutoReaderV2] === ç¬¬äºŒè½®ï¼šå†…å®¹ç†è§£ ===');
      const currentNotes = await fs.readFile(notesFilePath, 'utf-8');
      const pass2Prompt = PAPER_PASS_2_PROMPT.replace('{previous_notes}', currentNotes);
      const pass2Result = await this.executePass(tempFilePath, pass2Prompt, notesFilePath, 2);
      await this.appendToNotesFile(notesFilePath, '\n\n---\n\n## ç¬¬äºŒè½®ç¬”è®°\n\n' + cleanLLMResponse(pass2Result.text));

      // Step 4: Pass 3
      console.log('[AutoReaderV2] === ç¬¬ä¸‰è½®ï¼šæ·±åº¦ç†è§£ ===');
      const updatedNotes = await fs.readFile(notesFilePath, 'utf-8');
      const pass3Prompt = PAPER_PASS_3_PROMPT.replace('{previous_notes}', updatedNotes);
      const pass3Result = await this.executePass(tempFilePath, pass3Prompt, notesFilePath, 3);
      await this.appendToNotesFile(notesFilePath, '\n\n' + cleanLLMResponse(pass3Result.text));

      let finalNotes = await fs.readFile(notesFilePath, 'utf-8');

      // Step 5: Fetch README if has code
      if (hasCode && codeUrl) {
        console.log(`[AutoReaderV2] === è·å–å¹¶æ‘˜è¦ä»£ç README: ${codeUrl} ===`);
        try {
          const codeReadme = await this.fetchGitHubReadme(codeUrl);
          if (codeReadme) {
            const readmeSummary = await this.summarizeReadme(codeReadme, codeUrl, title);
            if (readmeSummary) {
              finalNotes += '\n\n---\n\n## ä»£ç ä»“åº“æ¦‚è§ˆ\n\n';
              finalNotes += `**ä»“åº“åœ°å€**: [${codeUrl}](${codeUrl})\n\n`;
              finalNotes += readmeSummary;
              finalNotes += '\n\n*ç‚¹å‡»"ä»£ç åˆ†æ"æŒ‰é’®è·å–è¯¦ç»†çš„ä»£ç è§£è¯»*\n';
            }
          }
        } catch (readmeError) {
          console.log(`[AutoReaderV2] README processing failed (non-fatal):`, readmeError.message);
        }
      }

      // Step 7: Upload notes to S3
      const paperNotesS3Key = await this.uploadNotesToS3(finalNotes, documentId, title, 'paper_notes');
      console.log(`[AutoReaderV2] Paper notes uploaded to S3: ${paperNotesS3Key}`);
      console.log(`[AutoReaderV2] Processing complete for: ${title}`);

      return {
        notesS3Key: paperNotesS3Key,
        codeNotesS3Key: null,
        pageCount: pdfInfo.pageCount,
        hasCode,
        codeUrl,
        readerMode: 'auto_reader_v2',
      };
    } finally {
      if (tempFilePath) {
        await pdfService.cleanupTmpFile(tempFilePath);
      }
      try {
        await fs.unlink(notesFilePath);
      } catch (e) { /* ignore */ }
    }
  }

  /**
   * Process document in v3 mode: 2-pass deep analysis with minimal implementation
   *
   * Pass 1 - è¡¨å±‚åˆ†æ:
   *   - ä»»åŠ¡å®šä¹‰ã€é¢†åŸŸç°çŠ¶ã€è¾“å…¥è¾“å‡ºè§„æ ¼ã€æ–¹æ³•æ¦‚è§ˆå›¾
   *
   * Pass 2 - æ·±å±‚åˆ†æ:
   *   - æœ€å°å¤ç°ä»£ç ã€æ•°å­¦æ¡†æ¶ã€æ‰¹åˆ¤æ€§åˆ†æã€æœªæ¥æ–¹å‘
   */
  async processDocumentV3(item, options = {}) {
    const { documentId, s3Key, title, analysisProvider } = item;
    let providedCodeUrl = item.codeUrl;
    let tempFilePath = null;
    const notesFilePath = path.join(this.processingDir, `${documentId}_notes.md`);
    this._currentProvider = this._resolveProvider(analysisProvider);

    try {
      await this.ensureProcessingDir();
      console.log(`[AutoReaderV3] Starting 2-pass deep analysis: ${title} (ID: ${documentId})`);

      // Step 1: Prepare PDF
      const pdfInfo = await pdfService.preparePdfForProcessing(s3Key);
      tempFilePath = pdfInfo.filePath;
      console.log(`[AutoReaderV3] PDF prepared: ${pdfInfo.pageCount} pages`);

      // Initialize notes file
      await this.initNotesFile(notesFilePath, title, documentId);

      // Step 2: Pass 1 - è¡¨å±‚åˆ†æ
      console.log('[AutoReaderV3] === ç¬¬ä¸€è½®ï¼šè¡¨å±‚åˆ†æ ===');
      const pass1Result = await this.executePass(tempFilePath, V3_PASS_1_PROMPT, notesFilePath, 1);
      await this.appendToNotesFile(notesFilePath, cleanLLMResponse(pass1Result.text));

      // Try to extract code URL from the analysis if mentioned
      let hasCode = !!providedCodeUrl;
      let codeUrl = providedCodeUrl;

      // Check if code URL is mentioned in pass 1 result
      const codeUrlMatch = pass1Result.text.match(/github\.com\/[^\s\)]+/i);
      if (codeUrlMatch && !codeUrl) {
        codeUrl = 'https://' + codeUrlMatch[0];
        hasCode = true;
      }

      // Step 3: Pass 2 - æ·±å±‚åˆ†æ
      console.log('[AutoReaderV3] === ç¬¬äºŒè½®ï¼šæ·±å±‚åˆ†æ ===');
      const currentNotes = await fs.readFile(notesFilePath, 'utf-8');
      const pass2Prompt = V3_PASS_2_PROMPT.replace('{previous_notes}', currentNotes);
      const pass2Result = await this.executePass(tempFilePath, pass2Prompt, notesFilePath, 2);
      await this.appendToNotesFile(notesFilePath, '\n\n' + cleanLLMResponse(pass2Result.text));

      let finalNotes = await fs.readFile(notesFilePath, 'utf-8');

      // Step 4: If has code, fetch README and summarize it (non-fatal)
      if (hasCode && codeUrl) {
        console.log(`[AutoReaderV3] === è·å–å¹¶æ‘˜è¦ä»£ç README: ${codeUrl} ===`);
        try {
          const codeReadme = await this.fetchGitHubReadme(codeUrl);
          if (codeReadme) {
            const readmeSummary = await this.summarizeReadme(codeReadme, codeUrl, title);
            if (readmeSummary) {
              finalNotes += '\n\n---\n\n## ä»£ç ä»“åº“æ¦‚è§ˆ\n\n';
              finalNotes += `**ä»“åº“åœ°å€**: [${codeUrl}](${codeUrl})\n\n`;
              finalNotes += readmeSummary;
              finalNotes += '\n\n*ç‚¹å‡»"ä»£ç åˆ†æ"æŒ‰é’®è·å–è¯¦ç»†çš„ä»£ç è§£è¯»*\n';
            }
          }
        } catch (readmeError) {
          console.log(`[AutoReaderV3] README processing failed (non-fatal):`, readmeError.message);
        }
      }

      // Step 5: Upload notes to S3
      const paperNotesS3Key = await this.uploadNotesToS3(finalNotes, documentId, title, 'paper_notes');
      console.log(`[AutoReaderV3] Paper notes uploaded to S3: ${paperNotesS3Key}`);
      console.log(`[AutoReaderV3] Processing complete for: ${title}`);

      return {
        notesS3Key: paperNotesS3Key,
        codeNotesS3Key: null,
        pageCount: pdfInfo.pageCount,
        hasCode,
        codeUrl,
        readerMode: 'auto_reader_v3',
      };
    } finally {
      if (tempFilePath) {
        await pdfService.cleanupTmpFile(tempFilePath);
      }
      try {
        await fs.unlink(notesFilePath);
      } catch (e) { /* ignore */ }
    }
  }

  /**
   * Parse pass 1 JSON result
   */
  parsePass1Result(text) {
    try {
      const jsonMatch = text.match(/```json\s*([\s\S]*?)\s*```/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[1]);
      }
    } catch (e) {
      console.warn('[AutoReader] Could not parse pass 1 JSON:', e.message);
    }
    return {
      title: '',
      paper_type: 'æœªçŸ¥',
      venue: 'æœªçŸ¥',
      has_code: false,
      code_url: null,
      core_contribution: '',
      key_pages: '',
      skip_pages: '',
      key_figures: [],
      five_c: {
        category: '',
        context: '',
        correctness: '',
        contributions: '',
        clarity: '',
      },
      initial_impression: '',
    };
  }

  /**
   * Append pass 1 notes in template format
   */
  async appendPass1Notes(filePath, data, rawText) {
    const notes = `
## æ¦‚è§ˆ

- **ç±»å‹**: ${data.paper_type || 'æœªçŸ¥'} | ${data.venue || ''}
- **ä»£ç **: ${data.has_code ? `[${data.code_url || 'æœ‰'}](${data.code_url || '#'})` : 'æ— '}
- **å…³é”®å›¾è¡¨**: ${data.key_figures?.length > 0 ? 'Figure ' + data.key_figures.join(', ') : 'å¾…åˆ†æ'}

### æ ¸å¿ƒè´¡çŒ®

${data.core_contribution || ''}

### è®ºæ–‡æ‘˜è¦

${data.summary || ''}

`;
    await this.appendToNotesFile(filePath, notes);
  }

  /**
   * Append reading log to notes (disabled - no longer needed)
   */
  async appendReadingLog(filePath) {
    // Reading log disabled - not useful for users
  }

  /**
   * Execute a single reading pass
   */
  /**
   * Resolve provider service from provider name
   */
  _resolveProvider(providerName) {
    switch (providerName) {
      case 'codex-cli': return codexCliService;
      case 'google-api': return googleApiService;
      case 'gemini-cli':
      default: return geminiCliService;
    }
  }

  async executePass(pdfPath, prompt, notesFilePath, passNumber) {
    const provider = this._currentProvider || geminiCliService;
    const providerName = this._currentProvider === codexCliService ? 'Codex CLI'
      : this._currentProvider === googleApiService ? 'Google API' : 'Gemini CLI';
    console.log(`[AutoReader] Executing pass ${passNumber} with ${providerName}...`);

    const result = await provider.readDocument(pdfPath, prompt);

    console.log(`[AutoReader] Pass ${passNumber} complete, output: ${result.text.length} chars`);

    return result;
  }

  /**
   * Initialize the notes file with template header
   */
  async initNotesFile(filePath, title, documentId) {
    const header = `# ${title}

`;
    await fs.writeFile(filePath, header, 'utf-8');
  }

  /**
   * Append content to notes file
   */
  async appendToNotesFile(filePath, content) {
    await fs.appendFile(filePath, content, 'utf-8');
  }

  /**
   * Extract excalidraw figures and convert to PNG
   */
  async extractAndConvertFigures(text, documentId) {
    const figures = [];
    // Match both naming conventions: excalidraw-name and excalidraw-paper_name/code_name
    const excalidrawPattern = /```excalidraw-(\w+)\s*([\s\S]*?)\s*```/g;
    let match;

    while ((match = excalidrawPattern.exec(text)) !== null) {
      const figureName = match[1];
      let figureJson = match[2].trim();

      // Try to parse as JSON
      try {
        // Sometimes the JSON is wrapped in code blocks
        if (figureJson.startsWith('{')) {
          const parsed = JSON.parse(figureJson);

          const excalidrawPath = path.join(this.processingDir, `${documentId}_${figureName}.excalidraw`);
          const pngPath = path.join(this.processingDir, `${documentId}_${figureName}.png`);

          await fs.writeFile(excalidrawPath, JSON.stringify(parsed, null, 2), 'utf-8');

          // Convert to PNG using the convert.py script
          const converted = await this.convertExcalidrawToPng(excalidrawPath, pngPath);

          figures.push({
            name: figureName,
            excalidrawPath,
            pngPath: converted ? pngPath : null,
            json: parsed,
          });
        }
      } catch (e) {
        console.warn(`[AutoReader] Could not parse excalidraw figure ${figureName}:`, e.message);
        figures.push({
          name: figureName,
          excalidrawPath: null,
          pngPath: null,
          json: null,
          error: e.message,
        });
      }
    }

    return figures;
  }

  /**
   * Convert excalidraw file to PNG
   */
  async convertExcalidrawToPng(excalidrawPath, pngPath) {
    return new Promise((resolve) => {
      const convertScript = path.join(__dirname, '..', '..', '..', 'docs', 'convert.py');

      const proc = spawn('python3', [convertScript, excalidrawPath, pngPath]);

      let stderr = '';
      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          console.log(`[AutoReader] Converted figure: ${pngPath}`);
          resolve(true);
        } else {
          console.warn(`[AutoReader] Figure conversion failed: ${stderr}`);
          resolve(false);
        }
      });

      proc.on('error', (err) => {
        console.warn(`[AutoReader] Figure conversion error: ${err.message}`);
        resolve(false);
      });
    });
  }

  /**
   * Generate final paper notes with embedded figures
   */
  async generateFinalPaperNotes(notesFilePath, figures, title, documentId) {
    let notes = await fs.readFile(notesFilePath, 'utf-8');

    // Add figure references section (only for successful conversions)
    const successfulFigures = figures.filter(f => f.pngPath);
    if (successfulFigures.length > 0) {
      notes += '\n\n---\n\n## å›¾è¡¨\n\n';
      for (const figure of successfulFigures) {
        const figureTitle = this.getFigureTitle(figure.name);
        notes += `### ${figureTitle}\n\n`;
        notes += `![${figureTitle}](figures/${documentId}_${figure.name}.png)\n\n`;
      }
    }

    return notes;
  }

  /**
   * Get human-readable figure title
   */
  getFigureTitle(name) {
    const titles = {
      'paper_outline': 'è®ºæ–‡ç»“æ„å›¾',
      'paper_method': 'æ–¹æ³•æµç¨‹å›¾',
      'repo_structure': 'ä»“åº“ç»“æ„å›¾',
      'code_method': 'ä»£ç æ¶æ„å›¾',
      // Legacy names
      'outline': 'è®ºæ–‡å¤§çº²å›¾',
      'method': 'æ–¹æ³•æµç¨‹å›¾',
      'structure': 'ä»£ç æ¶æ„å›¾',
      'implementation': 'å®ç°æµç¨‹å›¾',
    };
    return titles[name] || name;
  }

  /**
   * Analyze code repository
   */
  async analyzeCodeRepository(codeUrl, documentId, title) {
    const repoDir = path.join(this.processingDir, `repo_${documentId}`);
    const codeNotesPath = path.join(this.processingDir, `${documentId}_code_notes.md`);

    try {
      // Clone repository (skip LFS)
      console.log(`[AutoReader] Cloning repository: ${codeUrl}`);
      await this.cloneRepository(codeUrl, repoDir);
      console.log(`[AutoReader] Repository cloned to: ${repoDir}`);

      const now = new Date();
      // Initialize code notes with template header (no blockquote)
      await fs.writeFile(codeNotesPath, `---
title: ${title} - ä»£ç åˆ†æ
document_id: ${documentId}
code_url: ${codeUrl}
generated_at: ${now.toISOString()}
---

# ${title} - ä»£ç ç¬”è®°

**ä»“åº“åœ°å€**: [${codeUrl}](${codeUrl})

`, 'utf-8');

      // Use Claude Code CLI for code analysis (single comprehensive round)
      console.log('[AutoReader] Using Claude Code CLI for code analysis');

      // Single round: Comprehensive analysis
      console.log('[AutoReader] === ä»£ç åˆ†æ ===');
      const repoStructure = await this.getRepoStructure(repoDir);
      const analysisPrompt = CODE_ANALYSIS_PROMPT + '\n\n## ä»£ç ç›®å½•ç»“æ„:\n```\n' + repoStructure + '\n```';
      const analysisResult = await claudeCodeService.analyzeRepository(repoDir, analysisPrompt);
      await this.appendToNotesFile(codeNotesPath, '---\n\n' + analysisResult.text);

      // Extract and convert code figures
      const allText = analysisResult.text;
      const codeFigures = await this.extractAndConvertFigures(allText, `${documentId}_code`);

      // Add reading log
      const dateStr = new Date().toISOString().split('T')[0];
      await this.appendToNotesFile(codeNotesPath, `
---

## é˜…è¯»æ—¥å¿—

| æ—¥æœŸ | å¤‡æ³¨ |
|-----|------|
| ${dateStr} | è‡ªåŠ¨å¤„ç†å®Œæˆ |
`);

      // Generate final code notes
      const finalCodeNotes = await this.generateFinalCodeNotes(codeNotesPath, codeFigures, title, documentId);

      return finalCodeNotes;
    } finally {
      // Cleanup repo directory
      try {
        await fs.rm(repoDir, { recursive: true, force: true });
      } catch (e) { /* ignore */ }
      try {
        await fs.unlink(codeNotesPath);
      } catch (e) { /* ignore */ }
    }
  }

  /**
   * Clone a repository (skip LFS)
   */
  async cloneRepository(url, targetDir) {
    return new Promise((resolve, reject) => {
      const proc = spawn('git', [
        'clone',
        '--depth', '1',
        '--single-branch',
        url,
        targetDir,
      ], {
        env: { ...process.env, GIT_LFS_SKIP_SMUDGE: '1' },
      });

      let stderr = '';
      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Git clone failed: ${stderr}`));
        }
      });

      proc.on('error', reject);
    });
  }

  /**
   * Get repository structure using tree command
   */
  async getRepoStructure(repoDir) {
    return new Promise((resolve) => {
      // Try tree first for better output
      const treeProc = spawn('tree', ['-L', '3', '--noreport'], {
        cwd: repoDir,
      });

      let treeOutput = '';
      let treeError = false;

      treeProc.stdout.on('data', (data) => {
        treeOutput += data.toString();
      });

      treeProc.on('error', () => {
        treeError = true;
      });

      treeProc.on('close', (code) => {
        if (code === 0 && !treeError && treeOutput) {
          resolve(treeOutput.substring(0, 5000));
          return;
        }

        // Fallback to find
        const proc = spawn('find', ['.', '-type', 'f', '(',
          '-name', '*.py', '-o',
          '-name', '*.js', '-o',
          '-name', '*.ts', '-o',
          '-name', '*.json', '-o',
          '-name', '*.yaml', '-o',
          '-name', '*.yml', '-o',
          '-name', '*.md', '-o',
          '-name', 'requirements*.txt',
          ')', '-not', '-path', '*/.*'], {
          cwd: repoDir,
        });

        let output = '';
        proc.stdout.on('data', (data) => {
          output += data.toString();
        });

        proc.on('close', () => {
          // Limit to first 100 files
          const files = output.trim().split('\n').slice(0, 100);
          resolve(files.join('\n'));
        });

        proc.on('error', () => {
          resolve('Could not read repository structure');
        });
      });
    });
  }

  /**
   * Fetch README from GitHub repository
   * @param {string} codeUrl - GitHub repository URL
   * @returns {Promise<string|null>} - README content in markdown
   */
  async fetchGitHubReadme(codeUrl) {
    try {
      // Parse GitHub URL to get owner/repo
      const match = codeUrl.match(/github\.com\/([^\/]+)\/([^\/]+)/);
      if (!match) {
        console.log(`[AutoReader] Not a GitHub URL: ${codeUrl}`);
        return null;
      }

      const owner = match[1];
      const repo = match[2].replace(/\.git$/, '');

      // Try to fetch README via GitHub API
      const apiUrl = `https://api.github.com/repos/${owner}/${repo}/readme`;

      const https = require('https');

      return new Promise((resolve) => {
        const req = https.get(apiUrl, {
          headers: {
            'Accept': 'application/vnd.github.v3.raw',
            'User-Agent': 'auto-researcher'
          },
          timeout: 10000
        }, (res) => {
          if (res.statusCode !== 200) {
            console.log(`[AutoReader] README fetch failed: ${res.statusCode}`);
            resolve(null);
            return;
          }

          let data = '';
          res.on('data', chunk => data += chunk);
          res.on('end', () => {
            // Truncate if too long
            const maxLen = 3000;
            if (data.length > maxLen) {
              data = data.substring(0, maxLen) + '\n\n... (README å·²æˆªæ–­)';
            }
            resolve(data);
          });
        });

        req.on('error', (err) => {
          console.log(`[AutoReader] README fetch error: ${err.message}`);
          resolve(null);
        });

        req.on('timeout', () => {
          req.destroy();
          console.log(`[AutoReader] README fetch timeout`);
          resolve(null);
        });
      });
    } catch (e) {
      console.log(`[AutoReader] README fetch exception: ${e.message}`);
      return null;
    }
  }

  /**
   * Summarize README content using Gemini CLI
   * @param {string} readmeContent - Raw README content
   * @param {string} codeUrl - Repository URL
   * @param {string} paperTitle - Paper title for context
   * @returns {Promise<string|null>} - Summarized README
   */
  async summarizeReadme(readmeContent, codeUrl, paperTitle) {
    try {
      const prompt = `ä½ æ˜¯ä¸€ä½ä»£ç ä»“åº“åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹READMEå†…å®¹ï¼Œä¸ºè¿™ä¸ªä¸è®ºæ–‡"${paperTitle}"ç›¸å…³çš„ä»£ç ä»“åº“ç”Ÿæˆä¸€ä¸ªç®€æ´ä½†ä¿¡æ¯ä¸°å¯Œçš„æ¦‚è§ˆã€‚

## READMEå†…å®¹ï¼š
${readmeContent}

## è¾“å‡ºè¦æ±‚ï¼š
- ç”¨ä¸­æ–‡è¾“å‡º
- ç›´æ¥è¾“å‡ºMarkdownå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å¼€åœºç™½
- æå–æœ€å…³é”®çš„ä¿¡æ¯ï¼Œä¸è¦å†—ä½™

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

### é¡¹ç›®ç®€ä»‹
[1-2å¥è¯æè¿°é¡¹ç›®æ˜¯ä»€ä¹ˆï¼Œè§£å†³ä»€ä¹ˆé—®é¢˜]

### æ ¸å¿ƒç‰¹æ€§
- [ç‰¹æ€§1]
- [ç‰¹æ€§2]
- [ç‰¹æ€§3]

### å¿«é€Ÿå¼€å§‹
\`\`\`bash
[æœ€ç®€å•çš„å®‰è£…å’Œè¿è¡Œå‘½ä»¤ï¼Œå¦‚æœREADMEä¸­æœ‰çš„è¯]
\`\`\`

### æ¶æ„æ¦‚è§ˆ
\`\`\`
[ç”¨ASCIIå­—ç¬¦ç”»å‡ºé¡¹ç›®çš„æ ¸å¿ƒæ¶æ„ï¼Œæ ¹æ®READMEå†…å®¹æ¨æ–­]
ä¾‹å¦‚:
Input --> Module A --> Module B --> Output
           |              |
           +-- SubModule --+
\`\`\`

### ä¸è®ºæ–‡çš„å…³ç³»
[è¯´æ˜è¿™ä¸ªä»£ç ä»“åº“ä¸è®ºæ–‡çš„å¯¹åº”å…³ç³»ï¼Œå“ªäº›éƒ¨åˆ†å®ç°äº†è®ºæ–‡ä¸­çš„æ–¹æ³•]`;

      // Write prompt to temp file
      const promptPath = path.join(this.processingDir, `readme_prompt_${Date.now()}.txt`);
      await fs.writeFile(promptPath, prompt, 'utf-8');

      try {
        // Use current provider to summarize
        const provider = this._currentProvider || geminiCliService;
        const result = provider.runWithPromptFile
          ? await provider.runWithPromptFile(promptPath, { timeout: 60000 })
          : await provider.readMarkdown(prompt, '', { timeout: 60000 });
        return cleanLLMResponse(result.text);
      } finally {
        // Cleanup prompt file
        try {
          await fs.unlink(promptPath);
        } catch (e) { /* ignore */ }
      }
    } catch (error) {
      console.log(`[AutoReader] README summarization failed: ${error.message}`);
      // Fall back to truncated raw README
      return readmeContent.length > 1500
        ? readmeContent.substring(0, 1500) + '\n\n... (README å·²æˆªæ–­)'
        : readmeContent;
    }
  }

  /**
   * Generate final code notes with embedded figures
   */
  async generateFinalCodeNotes(notesFilePath, figures, title, documentId) {
    let notes = await fs.readFile(notesFilePath, 'utf-8');

    if (figures.length > 0) {
      notes += '\n\n---\n\n## å›¾è¡¨\n\n';
      for (const figure of figures) {
        const figureTitle = this.getFigureTitle(figure.name);
        if (figure.pngPath) {
          notes += `### ${figureTitle}\n\n`;
          notes += `![${figureTitle}](figures/${documentId}_code_${figure.name}.png)\n\n`;
        }
      }
    }

    return notes;
  }

  /**
   * Upload notes to S3
   */
  async uploadNotesToS3(notes, documentId, title, type) {
    const timestamp = Date.now();
    const sanitizedTitle = title.replace(/[^a-zA-Z0-9]/g, '_').substring(0, 50);
    const s3Key = `default_user/notes/${timestamp}-${documentId}-${sanitizedTitle}_${type}.md`;

    const buffer = Buffer.from(notes, 'utf-8');
    await s3Service.uploadBuffer(buffer, s3Key, 'text/markdown');

    return s3Key;
  }

  /**
   * Upload figure to S3
   */
  async uploadFigureToS3(pngPath, documentId, figureName) {
    try {
      const buffer = await fs.readFile(pngPath);
      const s3Key = `default_user/figures/${documentId}_${figureName}.png`;
      await s3Service.uploadBuffer(buffer, s3Key, 'image/png');
      return s3Key;
    } catch (e) {
      console.warn(`[AutoReader] Could not upload figure ${figureName}:`, e.message);
      return null;
    }
  }
}

module.exports = new AutoReaderService();
